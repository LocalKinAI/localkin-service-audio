# LocalKin Service Audio ğŸµ

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![uv](https://img.shields.io/badge/âš¡-uv-4c1d95)](https://github.com/astral-sh/uv)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## Local Speech-to-Text and Text-to-Speech with LocalKin Service Audio

**LocalKin Service Audio** simplifies local deployment of **Speech-to-Text (STT)** and **Text-to-Speech (TTS)** models. An intuitive **local audio** tool inspired by **Ollama's** simplicity - perfect for **local audio processing** workflows with both CLI and modern web interface support.

---

## âœ¨ Features

- **ğŸš€ Fast Startup**: Instant application launch with lazy loading architecture
- **âš¡ Faster-Whisper Integration**: Up to 4x faster transcription with CTranslate2 optimization
- **ğŸ¯ Multiple STT Engines**: OpenAI Whisper, faster-whisper, Ollama-based models, and Hugging Face models
- **ğŸ”Š Multiple TTS Engines**: Native OS TTS, Ollama-based conversational models, and SpeechT5/Bark
- **ğŸŒ REST API Server**: Run models as API servers with automatic endpoints
- **ğŸ’» Modern Web Interface**: Beautiful, responsive web UI for easy audio processing
- **ğŸ“¦ Smart Model Management**: Auto-pull models when needed, intelligent caching
- **ğŸ’¾ Persistent Cache**: Local model storage with size tracking and cleanup
- **ğŸ”„ Auto-Pull**: Models automatically download when running if not cached
- **ğŸ“Š Real-Time Status**: Live model status tracking with emoji indicators
- **ğŸ” Process Monitoring**: `kin audio ps` shows all running servers and their status
- **ğŸ“ˆ Model Transparency**: STT/TTS commands display detailed model information and statistics
- **âš¡ Performance Optimized**: Memory-efficient with GPU acceleration support
- **ğŸ¨ Professional Results**: High-quality audio processing with fine-tuned control
- **ğŸŒ CLI & Web**: Both command-line interface and modern web interface
- **ğŸ”§ Modular Architecture**: Clean, maintainable codebase with separated concerns

## ğŸš€ Quick Start

### Option 1: Install with uv (Recommended)

```bash
# Install uv if you haven't already
curl -LsSf https://astral.sh/uv/install.sh | sh

# Clone the repository
git clone https://github.com/LocalKinAI/localkin-service-audio.git
cd localkin-service-audio

# Install with uv (fast and reliable)
uv sync

# Now you can use kin directly
kin --help
```

### Option 2: Install with uv venv (Isolated Environment)

```bash
# Install uv if you haven't already
curl -LsSf https://astral.sh/uv/install.sh | sh

# Clone the repository
git clone https://github.com/LocalKinAI/localkin-service-audio.git
cd localkin-service-audio

# Create virtual environment and install
uv venv
source .venv/bin/activate  # Linux/macOS
# On Windows: .venv\Scripts\activate

# Install LocalKin Service Audio
uv sync

# Use kin
kin --help
```

### Option 3: Traditional pip Installation

```bash
# Clone the repository
git clone https://github.com/LocalKinAI/localkin-service-audio.git
cd localkin-service-audio

# Install dependencies (slower than uv)
pip install -e .

# Run commands using the module
kin --help
```

### Basic Usage

```bash
# Check version and help
kin --version
kin --help

# List all available models with status
kin audio models

# Speech-to-Text with local Whisper
kin audio transcribe audio.wav

# Real-time STT/TTS loop
kin audio listen                                    # Basic real-time transcription
kin audio listen --model faster-whisper-tiny        # Use specific STT model
kin audio listen --tts                               # Enable TTS with native voice
kin audio listen --tts --tts-model speecht5-tts     # Enable TTS with lightweight model

**ğŸ™ï¸ Real-time Listening Features:**
- **Smart Model Selection**: Choose any STT model (Whisper, faster-whisper variants)
- **TTS Model Options**: Select from native, Kokoro, SpeechT5, Bark, and more
- **Silence Detection**: Only processes audio with actual speech (reduces false positives)
- **TTS Cooldown**: 2-second cooldown prevents response spam
- **Smart Loading**: Lightweight models load at startup, complex models load on-demand
- **Error Recovery**: Graceful fallback to native TTS if advanced models fail
- **Automatic Engine Selection**: `faster-whisper-*` models use faster-whisper, others use OpenAI Whisper

**ğŸ’¡ For Complex TTS Models (Kokoro, Bark, XTTS):**

### ğŸš€ **Recommended: Use API Server (Best for Real-time)**
```bash
# Start TTS API server (models load once at startup)
kin audio run kokoro-82m --port 8001

# Use the API for instant TTS responses
curl -X POST "http://localhost:8001/synthesize" \
     -H "Content-Type: application/json" \
     -d '{"text": "Hello from real-time TTS!"}' \
     --output response.wav

# Your application can now call this API instantly
```

### âš ï¸ **CLI Real-time Mode Limitations**
```bash
# CLI mode loads models on-demand (slower, may timeout)
kin audio listen --tts --tts-model kokoro-82m

# Issues with CLI mode:
# - Models load during first TTS call (30+ seconds)
# - May timeout on slower systems
# - Not suitable for real-time applications
```

### ğŸ“Š **Performance Comparison**

| Method | Startup Time | First TTS | Memory | Reliability |
|--------|-------------|-----------|--------|-------------|
| **API Server** | 5-10s âš¡ | Instant âš¡ | High (persistent) | Excellent âœ… |
| **CLI Listen** | 2s âš¡ | 30-60s ğŸŒ | Moderate | Limited âš ï¸ |

**ğŸ¯ Bottom Line: For real-time TTS applications, always use the API server approach!**

# Text-to-Speech with native engine
kin audio tts "Hello, world!"

# ğŸš€ NEW: Run models as API servers (auto-pulls if needed)
kin audio run faster-whisper-tiny --port 8000   # Fast STT
kin audio run kokoro-82m --port 8001            # High-quality TTS
kin audio run speecht5-tts --port 8002          # Fast TTS

# ğŸš€ NEW: Monitor running servers and processes
kin audio ps                      # Show all running API servers
kin audio ps --verbose            # Detailed server information

# Check system status and cache
kin audio status
kin audio cache info

# Manage model cache
kin audio cache clear whisper-tiny-hf  # Clear specific model
kin audio cache clear                   # Clear all cached models
```

### âš¡ Performance & Benchmarks

LocalKin Service Audio provides **dual-engine transcription** with intelligent performance optimization:

```bash
# Automatic engine selection (recommended - adapts to your hardware/audio)
kin audio transcribe audio.wav

# Force specific engines
kin audio transcribe audio.wav --engine faster     # Force faster-whisper
kin audio transcribe audio.wav --engine openai     # Force OpenAI Whisper

# Check available engines and hardware
kin audio status
```

#### ğŸ“Š **Comprehensive Performance Analysis**

##### **Engine Comparison Matrix**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Use Case       â”‚ Best Engine     â”‚ Hardware        â”‚ Performance     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Short audio    â”‚ OpenAI Whisper  â”‚ CPU (any)       â”‚ Fastest         â”‚
â”‚ (<5 min)       â”‚                 â”‚                 â”‚                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Long audio     â”‚ faster-whisper  â”‚ CUDA GPU        â”‚ 4x faster       â”‚
â”‚ (>5 min)       â”‚                 â”‚                 â”‚                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Mac mini       â”‚ OpenAI Whisper  â”‚ Apple Silicon   â”‚ Most reliable   â”‚
â”‚ (MPS)          â”‚                 â”‚                 â”‚                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Intel Mac      â”‚ faster-whisper  â”‚ NVIDIA GPU      â”‚ Maximum speed   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Auto-select    â”‚ --engine auto   â”‚ Any hardware    â”‚ Smart choice    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

##### **Official Benchmark Data** (Source: [SYSTRAN/faster-whisper](https://github.com/SYSTRAN/faster-whisper))

**Test Conditions:** Small model on CPU, long-form audio content

| Implementation | Precision | Beam Size | Time | RAM Usage | Speedup |
|----------------|-----------|-----------|------|-----------|---------|
| **openai/whisper** | fp32 | 5 | **6m58s** | 2335MB | 1x |
| whisper.cpp | fp32 | 5 | 2m05s | 1049MB | 3.3x |
| whisper.cpp (OpenVINO) | fp32 | 5 | 1m45s | 1642MB | 4x |
| **faster-whisper** | fp32 | 5 | **2m37s** | 2257MB | **2.7x** |
| **faster-whisper** (batch_size=8) | fp32 | 5 | **1m06s** | 4230MB | **6.3x** |
| **faster-whisper** | int8 | 5 | **1m42s** | 1477MB | **4.8x** |
| **faster-whisper** (batch_size=8) | int8 | 5 | **51s** | 3608MB | **13.7x** |

##### **Our Test Results** (52.75s audio file)

| Engine | Model Size | Hardware | Time | Real-time Speed |
|--------|------------|----------|------|-----------------|
| **OpenAI Whisper** | Small (244MB) | CPU | ~6.8s | **13x** |
| **faster-whisper** | Small (244MB) | CPU | ~15.7s | **3.4x** |
| **OpenAI Whisper** | Medium (769MB) | CPU | ~19.9s | **2.6x** |
| **faster-whisper** | Medium (769MB) | CPU | ~80.7s | **0.65x** |

#### ğŸ¯ **Performance Insights**

##### **Why the Discrepancy?**
- **Benchmark**: Tests long-form content (50+ minutes) where batched inference excels
- **Our Tests**: Short audio (52s) where initialization overhead dominates
- **Hardware**: Benchmark assumes CUDA GPU; our tests on CPU-only Mac mini

##### **Adaptive Intelligence**
LocalKin Service Audio automatically chooses the optimal approach:

- **Short Audio** (<5 minutes): Standard inference (less overhead)
- **Long Audio** (>5 minutes): Batched inference (maximum throughput)
- **Hardware Detection**: Adapts to available GPU acceleration

##### **GPU Acceleration Support**

| Hardware | OpenAI Whisper | faster-whisper | Notes |
|----------|----------------|----------------|--------|
| **CUDA GPU** | âœ… Supported | âœ… **4x speedup** | Best performance |
| **Apple MPS** | âš ï¸ Limited | âŒ Unsupported | Use OpenAI Whisper |
| **CPU-only** | âœ… Works | âœ… Works | OpenAI often faster |

#### ğŸš€ **Performance Optimization Tips**

##### **For Maximum Speed:**
```bash
# Long audio on CUDA systems
kin audio transcribe long_audio.wav --engine faster --model faster-whisper-large-v3

# Short audio on any system
kin audio transcribe short_audio.wav --engine openai --model_size tiny

# Auto-optimization (recommended)
kin audio transcribe audio.wav  # Automatically chooses best engine
```

##### **Model Selection Guide:**

| Model | Size | Best For | Speed | Quality | Status |
|-------|------|----------|-------|---------|--------|
| `tiny` | 39MB | Fast transcription | 32x | Basic | âœ… Working |
| `base` | 74MB | Balance | 16x | Good | âœ… Working |
| `small` | 244MB | Quality priority | 8x | High | âœ… Working |
| `medium` | 769MB | High accuracy | 4x | Very High | âœ… Working |
| `large-v3` | 3GB | Maximum accuracy | 1x | Excellent | âœ… Working |

**TTS Models:**

| Model | Size | Quality | Status | Notes |
|-------|------|---------|--------|-------|
| `native` | ~10MB | Basic | âœ… Working | System TTS |
| `speecht5-tts` | 1300MB | High | âœ… Working | Neural TTS |
| `kokoro-82m` | 320MB | High | âš ï¸ API Only | Complex model |
| `bark-small` | ~1GB | Very High | ğŸš§ Limited | Experimental |

#### âš¡ **Faster-Whisper Exclusive Features**

- **Automatic VAD Filtering**: Removes silence for cleaner results
- **Language Detection**: Auto-detects spoken language with confidence scores
- **Batched Processing**: Optimized for multiple files and long content
- **Memory Efficient**: Lower RAM usage with int8 quantization
- **GPU Optimized**: CUDA acceleration when available

#### ğŸ’¡ **When to Use Each Engine**

##### **Use OpenAI Whisper:**
- âœ… Short audio files (<5 minutes)
- âœ… Apple Silicon Macs (MPS has limitations)
- âœ… CPU-only systems
- âœ… Maximum compatibility
- âœ… When faster-whisper unavailable

##### **Use faster-whisper:**
- âœ… Long audio files (>5 minutes)
- âœ… CUDA GPU systems available
- âœ… Batch processing multiple files
- âœ… Language detection needed
- âœ… Memory-constrained environments

**ğŸ¯ Bottom Line: Auto-selection (`--engine auto`) gives you optimal performance for any scenario!**

### ğŸ’» Web Interface Quick Start

```bash
# Launch the modern web interface
kin web

# Launch on specific port
kin web --port 8080

# Launch and bind to all interfaces
kin web --host 0.0.0.0 --port 8080

# Open http://localhost:8080 in your browser
```

**Features:**
- ğŸ¨ Modern, responsive web interface
- ğŸ“¤ Drag & drop file upload for transcription
- ğŸ”Š Real-time audio playback for generated speech
- ğŸ“Š Live progress tracking and status updates
- ğŸ’¾ Automatic file downloads
- ğŸ¯ Model selection and configuration
- ğŸŒ Works on any device with a browser

### ğŸš€ API Server Quick Start

```bash
# Start a Whisper STT server (auto-pulls if needed)
kin audio run whisper-tiny-hf --port 8000

# Server starts at http://localhost:8000
# API docs: http://localhost:8000/docs

# Use the API (in another terminal)
curl -X POST "http://localhost:8000/transcribe" \
     -H "accept: application/json" \
     -H "Content-Type: multipart/form-data" \
     -F "file=@audio.wav"

# Response:
# {
#   "text": "Hello, this is a test transcription",
#   "language": "en",
#   "confidence": 0.95
# }
```

### ğŸ” Enhanced STT & TTS with Model Details

#### STT with Detailed Model Information
```bash
$ kin audio transcribe audio.wav --model_size large

ğŸµ LocalKin Service Audio - Local STT & TTS Model Manager
==================================================
â„¹ï¸  ğŸµ Transcribing audio file: audio.wav
â„¹ï¸  ğŸ¤– Using Whisper model: large
â„¹ï¸  ğŸ“Š Model details: 1550MB | 1x speed | Excellent quality
â„¹ï¸  ğŸ”„ Processing audio...

âœ… âœ… Transcription complete!
ğŸ“ Transcription Result:
============================================================
[transcription text here]
============================================================
â„¹ï¸  ğŸ“Š Statistics: 42 words, 256 characters
```

#### TTS with Engine Information
```bash
$ kin audio tts "Hello, this is a test" --output test.wav

ğŸµ LocalKin Service Audio - Local STT & TTS Model Manager
==================================================
â„¹ï¸  ğŸ”Š Synthesizing speech...
â„¹ï¸  ğŸ¤– Using TTS engine: pyttsx3 (native OS TTS)
â„¹ï¸  ğŸ“ Text: Hello, this is a test
â„¹ï¸  ğŸ“Š Text statistics: 6 words, 21 characters
â„¹ï¸  ğŸ’¾ Output file: test.wav
â„¹ï¸  ğŸµ Output format: WAV (uncompressed)
â„¹ï¸  ğŸ”„ Processing text...

âœ… âœ… Speech synthesized and saved to: test.wav
â„¹ï¸  File size: 0.08MB
```

#### Monitor Running Servers
```bash
$ kin audio ps

ğŸµ LocalKin Service Audio - Local STT & TTS Model Manager
==================================================
â„¹ï¸  Checking for running LocalKin Service Audio processes...
âœ… Found 2 running LocalKin Service Audio server(s):

================================================================================
PORT     MODEL                     TYPE     URL                       STATUS
================================================================================
8000     whisper-tiny-hf           stt      http://localhost:8000     ğŸŸ¢ Running
8001     speecht5-tts              tts      http://localhost:8001     ğŸŸ¢ Running
================================================================================

ğŸ’¡ Tip: Access interactive API docs at http://localhost:<PORT>/docs
```

---

## ğŸ¯ Supported Models

Choose from **20 state-of-the-art audio processing models** covering the latest in speech technology:

### ğŸ“Š Model Status Overview

```bash
$ kin audio models
MODEL                          TYPE   STATUS             SOURCE          DESCRIPTION
------------------------------------------------------------------------------------------
whisper                        stt    ğŸ“¦ Local Library    openai-whisper  OpenAI Whisper with auto faster-whisper
faster-whisper-tiny            stt    ğŸ“¦ Local Library    faster-whisper  Fast Whisper Tiny - 4x faster, smallest size
faster-whisper-base            stt    ğŸ“¦ Local Library    faster-whisper  Fast Whisper Base - 4x faster, good balance
faster-whisper-small           stt    ğŸ“¦ Local Library    faster-whisper  Fast Whisper Small - 4x faster, high quality
faster-whisper-medium          stt    ğŸ“¦ Local Library    faster-whisper  Fast Whisper Medium - 4x faster, very high quality
faster-whisper-large-v3        stt    ğŸ“¦ Local Library    faster-whisper  Fast Whisper Large v3 - 4x faster, best quality
faster-whisper-turbo           stt    ğŸ“¦ Local Library    faster-whisper  Fast Whisper Turbo - fastest, optimized for speed
whisper-tiny-hf                stt    âœ… Pulled           huggingface     Whisper Tiny from Hugging Face
whisper-base-hf                stt    âœ… Pulled           huggingface     Whisper Base from Hugging Face
whisper-large-v2-hf            stt    â¬‡ï¸ Not Pulled      huggingface     Whisper Large v2 from Hugging Face
------------------------------------------------------------------------------------------
native                         tts    ğŸ“¦ Local Library    pyttsx3         Native macOS TTS via pyttsx3
speecht5-tts                   tts    â¬‡ï¸ Not Pulled      huggingface     Microsoft SpeechT5 TTS from Hugging Face
bark-small                     tts    â¬‡ï¸ Not Pulled      huggingface     Suno Bark Small TTS from Hugging Face
kokoro-82m                     tts    âœ… Pulled           huggingface     HexGrad Kokoro-82M - high-quality neural TTS
xtts-v2                        tts    â¬‡ï¸ Not Pulled      huggingface     Coqui XTTS v2 - multilingual voice cloning TTS
mms-tts-eng                    tts    â¬‡ï¸ Not Pulled      huggingface     Meta MMS TTS English - multilingual
tortoise-tts                   tts    â¬‡ï¸ Not Pulled      huggingface     Tortoise TTS - high-quality multi-speaker

### ğŸ“Š Model Status Indicators

| Status | Meaning | Ready to Use? | Speed |
|--------|---------|---------------|-------|
| **ğŸ“¦ Local Library** | Model is part of installed Python packages, downloads on first use | âœ… **Yes** - Instant | âš¡ Fastest |
| **âœ… Pulled** | Model downloaded and cached locally | âœ… **Yes** - Instant | âš¡ Fast |
| **â¬‡ï¸ Not Pulled** | Model available but needs downloading first | âŒ **No** - Downloads first | ğŸŒ Slower first time |

**ğŸ’¡ Tip:** Models with "Local Library" status (like faster-whisper) are your fastest options!
```

### ğŸ“‹ Model Specifications

| Model | Type | Source | License | Quality | Speed | Size | Status |
|-------|------|--------|---------|---------|-------|------|---------|
| **whisper** | STT | Local | MIT | High | Fast | 139MB-1550MB | âœ… Ready |
| **whisper-tiny-hf** | STT | Hugging Face | MIT | Basic | 32x | 39MB | âœ… Cached |
| **whisper-base-hf** | STT | Hugging Face | MIT | Good | 16x | 290MB | âœ… Cached |
| **whisper-large-v2-hf** | STT | Hugging Face | MIT | Excellent | 1x | 2.87GB | â¬‡ï¸ Not Pulled |
| **speecht5-tts** | TTS | Hugging Face | MIT | High | Medium | 1300MB | â¬‡ï¸ Not Pulled |
| **bark-small** | TTS | Hugging Face | MIT | Very High | Slow | 1600MB | â¬‡ï¸ Not Pulled |
| **native** | TTS | Local | MIT | Good | Instant | Local | âœ… Ready |
| **whisper-large-v3** | STT | Ollama | Apache 2.0 | Very High | Medium | 1550MB | â¬‡ï¸ Not Pulled |
| **whisper-base** | STT | Ollama | Apache 2.0 | Good | Very Fast | 139MB | â¬‡ï¸ Not Pulled |
| **llama3.2:3b** | TTS | Ollama | Llama License | High | Medium | 1820MB | â¬‡ï¸ Not Pulled |
| **qwen2.5:3b** | TTS | Ollama | Apache 2.0 | High | Medium | 1820MB | â¬‡ï¸ Not Pulled |
| **mistral:7b** | TTS | Ollama | Apache 2.0 | Very High | Slow | 3820MB | â¬‡ï¸ Not Pulled |

### STT Models - Speech-to-Text

#### ğŸš€ Hugging Face Models (API Server - Recommended)
```bash
# Start API server (auto-pulls if needed)
kin audio run whisper-tiny-hf --port 8000    # Fastest, 39MB
kin audio run whisper-base-hf --port 8001    # Balanced, 290MB
kin audio run whisper-large-v2-hf --port 8002 # Best quality, 2.87GB

# Use the API
curl -X POST "http://localhost:8000/transcribe" \
     -H "Content-Type: multipart/form-data" \
     -F "file=@audio.wav"

# Interactive API docs: http://localhost:8000/docs
```

#### Local Whisper Models (Direct CLI - No API Key Required)
```bash
# Use different Whisper model sizes (shows detailed model info)
kin audio transcribe audio.wav --model_size tiny    # 39MB, 32x speed, Basic quality
kin audio transcribe audio.wav --model_size base    # 74MB, 16x speed, Good quality
kin audio transcribe audio.wav --model_size small   # 244MB, 8x speed, High quality
kin audio transcribe audio.wav --model_size medium  # 769MB, 4x speed, Very High quality
kin audio transcribe audio.wav --model_size large   # 1550MB, 1x speed, Excellent quality

# Each command displays:
# - Model size, speed, and quality details
# - Processing progress with download status
# - Transcription results with word/char statistics
```

#### Ollama Whisper Models (Requires Ollama)
```bash
# Pull models first
kin audio pull whisper-large-v3
kin audio pull whisper-base

# Use them (future feature)
kin audio transcribe audio.wav --model whisper-large-v3
```

### TTS Models - Text-to-Speech

#### ğŸš€ Hugging Face Models (API Server - Recommended)
```bash
# Start TTS API server (auto-pulls if needed)
kin audio run kokoro-82m --port 8001      # HexGrad Kokoro, 320MB (best quality)
kin audio run xtts-v2 --port 8002         # Coqui XTTS v2, 1.8GB (voice cloning)
kin audio run speecht5-tts --port 8003    # Microsoft SpeechT5, 250MB
kin audio run bark-small --port 8004      # Suno Bark, 1.7GB

# Use the API for speech synthesis
curl -X POST "http://localhost:8003/synthesize" \
     -H "Content-Type: application/json" \
     -d '{
       "text": "Hello, this is generated speech!",
       "voice": "af_sarah"
     }' \
     --output speech.wav

# Interactive API docs: http://localhost:8001/docs
```

#### âš ï¸ Direct CLI TTS Models (Limited)
```bash
# Native TTS (works immediately)
kin audio tts "Hello world" --model native

# Advanced TTS (âš ï¸ CLI may timeout - use API server instead)
# kin audio tts "Hello world" --model kokoro-82m  # May timeout - use API server below
# kin audio tts "Hello world" --model xtts-v2     # May timeout - use API server below
```

#### Native OS TTS (Works Immediately)
```bash
# Basic usage (shows engine and text details)
kin audio tts "Hello, world!"

# Save to file (shows format and file size info)
kin audio tts "This is a test" --output hello.wav
kin audio tts "Long text here..." --output speech.mp3

# Each command displays:
# - TTS engine information (pyttsx3/native OS)
# - Text statistics (word count, character count)
# - Output file format detection
# - Generated audio file size
# - Processing progress
```

#### Ollama Conversational Models (Future Feature)
```bash
# Pull conversational models for TTS
kin audio pull llama3.2:3b-instruct-q4_0
kin audio pull qwen2.5:3b-instruct-q4_0

# Use them for conversational TTS (future feature)
kin audio tts "Tell me a story" --model llama3.2:3b-instruct-q4_0
```

---

## ğŸ“¦ Installation & Setup

### Prerequisites

- **Python 3.10+** (required for optimal performance)
- **uv** (fast Python package installer - highly recommended)
- **Ollama** (optional, for Ollama-based models)
- **FFmpeg** (for audio processing)

### Quick Install with uv

```bash
# Install uv (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Clone and install
git clone https://github.com/LocalKinAI/localkin-service-audio.git
cd localkin-service-audio

# Install with uv (fast and reliable)
uv sync
```

### Ollama Setup (Optional)

For advanced models, install Ollama:

```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Start Ollama service
ollama serve
```

### Audio Processing Setup

For audio file processing, ensure you have audio libraries (all included in pyproject.toml):

```bash
# Install additional audio processing dependencies if needed
uv pip install librosa soundfile

# For macOS, you might need additional audio support
# All necessary dependencies are included in pyproject.toml
```

### Optional Dependencies

LocalKin Service Audio supports optional features that can be installed separately:

```bash
# Install with web interface support
uv sync --extra web

# Install with advanced TTS models (XTTS, etc.)
uv sync --extra tts

# Install with GPU acceleration support
uv sync --extra gpu

# Install with all optional features
uv sync --extra web --extra tts --extra gpu
```

**Web Interface (`--extra web`):**
- Modern web-based UI for audio processing
- Responsive design that works on all devices
- Real-time progress tracking and file downloads
- Requires: `jinja2` for templating

**Advanced TTS (`--extra tts`):**
- Coqui XTTS v2 for high-quality voice cloning
- Multilingual speech synthesis capabilities
- Voice cloning and customization features
- Requires: `TTS` package from Coqui

**GPU Support (`--extra gpu`):**
- CUDA acceleration for faster model inference
- Automatic GPU detection and optimization
- Requires compatible NVIDIA GPU and drivers

---

## ğŸ›ï¸ Command Reference

### Core Commands

```bash
# Display help
kin --help

# Show version
kin --version

# List all available models with status
kin audio models

# Check system status
kin audio status

# ğŸš€ NEW: Monitor running servers and processes
kin audio ps

# ğŸ’» NEW: Launch web interface
kin web                              # Launch on default port 8080
kin web --port 3000                  # Launch on custom port
```

### ğŸš€ Model Server Commands

```bash
# Run model as API server (auto-pulls if needed)
kin audio run whisper-tiny-hf --port 8000     # STT server
kin audio run speecht5-tts --port 8001       # TTS server

# Monitor running servers
kin audio ps                                 # Show all active servers

# Server endpoints:
# - http://localhost:8000/           # API info
# - http://localhost:8000/health     # Health check
# - http://localhost:8000/docs       # Interactive API docs
# - http://localhost:8000/transcribe # STT endpoint
# - http://localhost:8000/synthesize # TTS endpoint
```

### Model Management

```bash
# Pull models (Ollama or Hugging Face)
kin audio pull whisper-large-v3              # Ollama model
kin audio pull whisper-tiny-hf               # Hugging Face model

# Cache management
kin audio cache info                         # Show cache status
kin audio cache clear whisper-tiny-hf       # Clear specific model
kin audio cache clear                       # Clear all cached models
```

### Speech-to-Text

```bash
# Basic transcription (shows detailed model info)
kin audio transcribe audio.wav

# Specify model size for Whisper (shows size/speed/quality details)
kin audio transcribe audio.wav --model_size tiny     # 39MB, 32x speed, Basic quality
kin audio transcribe audio.wav --model_size base     # 74MB, 16x speed, Good quality
kin audio transcribe audio.wav --model_size small    # 244MB, 8x speed, High quality
kin audio transcribe audio.wav --model_size medium   # 769MB, 4x speed, Very High quality
kin audio transcribe audio.wav --model_size large    # 1550MB, 1x speed, Excellent quality

# Choose transcription engine (auto-selects faster-whisper when available)
kin audio transcribe audio.wav --engine auto         # Auto-select best engine (default)
kin audio transcribe audio.wav --engine faster       # Force faster-whisper (best for long audio/CUDA)
kin audio transcribe audio.wav --engine openai       # Force OpenAI Whisper (compatible)

# Use specific faster-whisper models (recommended for best performance)
kin audio transcribe audio.wav --model faster-whisper-tiny      # Fastest, lowest quality
kin audio transcribe audio.wav --model faster-whisper-base      # Good balance
kin audio transcribe audio.wav --model faster-whisper-small     # High quality
kin audio transcribe audio.wav --model faster-whisper-large-v3  # Best quality, slower

# Save transcription to file
kin audio transcribe audio.wav > transcription.txt

# Output includes:
# - Engine selection (OpenAI Whisper vs faster-whisper)
# - Model details (size, speed, quality)
# - Language detection (faster-whisper only)
# - Processing progress
# - Transcription statistics (word/char count)
```

### Text-to-Speech

```bash
# Basic speech synthesis (shows engine details)
kin audio tts "Hello, world!"

# Save to audio file (shows format and file size)
kin audio tts "This is a test message" --output test.wav
kin audio tts "Hello" --output greeting.mp3

# Read from file
kin audio tts "$(cat script.txt)" --output narration.wav

# Output includes:
# - TTS engine information (pyttsx3/native OS)
# - Text statistics (word/char count)
# - Output format detection (WAV/MP3)
# - Generated file size
# - Processing progress
```

---

## ğŸ’¾ Cache Management

LocalKin Service Audio uses intelligent caching to store downloaded models locally for faster subsequent loads.

### Cache Location

Models are cached in: `~/.localkin_service_audio/cache/huggingface/`

### Cache Commands

```bash
# View cache status
kin audio cache info

# Output:
# ğŸ“¦ Cached Models (2):
#   â€¢ whisper-tiny-hf (580.65MB)
#   â€¢ whisper-base-hf (1112.27MB)

# Clear specific model from cache
kin audio cache clear whisper-tiny-hf

# Clear all cached models
kin audio cache clear

# Check cache directory size
du -sh ~/.localkin_service_audio/cache/
```

### Auto-Pull Behavior

When you run a model that isn't cached:

```bash
kin audio run whisper-large-v2-hf --port 8000
# Output:
# ğŸ“¥ Model 'whisper-large-v2-hf' not found in cache. Pulling it first...
# ğŸ“¥ Downloading model from Hugging Face: openai/whisper-large-v2
# âœ… Model downloaded to: /Users/.../whisper-large-v2-hf
# âœ… Model pulled successfully!
# ğŸš€ Starting LocalKin Service Audio API server for whisper-large-v2-hf
```

### Cache Benefits

- **ğŸš€ Fast Startup**: Pre-downloaded models load instantly
- **ğŸ’¾ Space Efficient**: Only cache models you actually use
- **ğŸ”„ Auto Management**: Automatic downloading when needed
- **ğŸ“Š Status Tracking**: Real-time cache status in `list` command

---

## âš™ï¸ Configuration

### Model Configuration

Models are configured in `localkin_service_audio/models.json`. The system supports:

- **Local models**: OpenAI Whisper, pyttsx3 (native TTS)
- **Hugging Face models**: Transformers-based STT/TTS models
- **Ollama models**: Whisper variants, conversational models
- **Custom models**: Easy to add new model configurations

### Configuration File Structure

```json
{
  "models": [
    {
      "name": "whisper-tiny-hf",
      "type": "stt",
      "description": "OpenAI Whisper Tiny model from Hugging Face",
      "source": "huggingface",
      "huggingface_repo": "openai/whisper-tiny",
      "license": "MIT",
      "requirements": ["transformers", "torch"],
      "sizes": ["39MB"],
      "tags": ["fast", "lightweight"]
    },
    {
      "name": "speecht5-tts",
      "type": "tts",
      "description": "Microsoft SpeechT5 TTS model from Hugging Face",
      "source": "huggingface",
      "huggingface_repo": "microsoft/speecht5_tts",
      "license": "MIT",
      "requirements": ["transformers", "torch"],
      "sizes": ["250MB"],
      "tags": ["high-quality", "multi-speaker"]
    }
  ],
  "metadata": {
    "version": "1.0.0",
    "last_updated": "2024-12-19"
  }
}
```

### ğŸš€ Adding New Models (Super Easy!)

LocalKin Service Audio makes adding new models incredibly simple! Here's how:

#### **Method 1: Quick Add (Most Common)**

```bash
# For Hugging Face models - just add to models.json!
# Edit localkin_service_audio/models.json and add your model:

{
  "name": "your-new-model",
  "type": "stt",  // or "tts"
  "description": "Your model description",
  "source": "huggingface",
  "huggingface_repo": "organization/model-name",
  "license": "MIT",
  "size_mb": 500,
  "requirements": ["transformers", "torch"],
  "tags": ["your", "tags"]
}

# That's it! The system handles the rest automatically.
```

#### **Method 2: CLI Helper (Now Available!)**

```bash
# List available templates
kin audio list-templates

# Add model using template (easiest!)
kin audio add-model --template whisper_stt --name my-whisper-model

# Add custom Hugging Face model
kin audio add-model --repo openai/whisper-medium --name whisper-medium --type stt

# Add with custom description and size
kin audio add-model --repo microsoft/speecht5_tts --name speecht5 --type tts \
                     --description "Microsoft's advanced neural TTS" \
                     --size-mb 1300
```

### ğŸ¯ **Supported Model Sources**

| Source | Description | Auto-Pull | API Server | Easy to Add |
|--------|-------------|-----------|------------|-------------|
| `huggingface` | ğŸ¤— Hugging Face Hub models | âœ… Yes | âœ… Yes | â­â­â­â­â­ |
| `ollama` | ğŸ¦™ Ollama models | âœ… Yes | ğŸš§ Future | â­â­â­â­ |
| `openai-whisper` | ğŸµ Local Whisper models | âŒ N/A | âŒ No | â­â­â­â­â­ |
| `pyttsx3` | ğŸ—£ï¸ Native OS TTS | âŒ N/A | âŒ No | â­â­â­â­â­ |

### ğŸ“‹ **Model Configuration Template**

```json
{
  "name": "your-model-name",
  "type": "stt",                    // "stt" or "tts"
  "description": "Brief description of your model",
  "source": "huggingface",         // "huggingface", "ollama", etc.
  "huggingface_repo": "org/model", // For Hugging Face models
  "license": "MIT",                // Model license
  "size_mb": 500,                  // Approximate size in MB
  "requirements": [                // Python packages needed
    "transformers",
    "torch"
  ],
  "tags": [                        // Optional tags for filtering
    "high-quality",
    "multilingual"
  ]
}
```

### ğŸ”§ **Advanced: Custom Model Types**

For models needing special handling, you can:

1. **Add new source type** in `validate_model_config()` in `config.py`
2. **Add loading logic** in `server.py` for new model types
3. **Update requirements** in `pyproject.toml` if needed

### ğŸ¯ **Available Templates**

LocalKin Service Audio comes with pre-built templates for popular models:

#### **STT Templates:**
- `whisper_stt` - OpenAI Whisper models (tiny, base, small, medium, large)
- `wav2vec2_stt` - Facebook Wav2Vec2 models
- `hubert_stt` - Facebook HuBERT models

#### **TTS Templates:**
- `speecht5_tts` - Microsoft SpeechT5 models
- `bark_tts` - Suno Bark models
- `fastspeech2_tts` - ESPnet FastSpeech2 models
- `tacotron2_tts` - ESPnet Tacotron2 models

#### **View All Templates:**
```bash
kin audio list-templates
```

### ğŸ“š **Examples of Easy Additions**

#### **Add a New Whisper Model:**
```json
{
  "name": "whisper-medium-hf",
  "type": "stt",
  "description": "OpenAI Whisper Medium model from Hugging Face.",
  "source": "huggingface",
  "huggingface_repo": "openai/whisper-medium",
  "license": "MIT",
  "size_mb": 1500,
  "requirements": ["transformers", "torch"],
  "tags": ["medium", "balanced", "huggingface"]
}
```

#### **Add a New TTS Model:**
```json
{
  "name": "tts-model-x",
  "type": "tts",
  "description": "Amazing new TTS model.",
  "source": "huggingface",
  "huggingface_repo": "company/amazing-tts",
  "license": "Apache 2.0",
  "size_mb": 800,
  "requirements": ["transformers", "torch"],
  "tags": ["neural", "high-quality"]
}
```

### âœ… **Test Your New Model**

```bash
# 1. Check if it appears in the list
kin audio models

# 2. Run it as a server (auto-pulls if needed)
kin audio run your-new-model --port 8000

# 3. Test the API
curl -X POST "http://localhost:8000/transcribe" \
     -F "file=@test.wav"
```

### ğŸ¯ **Why It's So Easy**

1. **ğŸ“ JSON Configuration**: No code changes needed for most models
2. **ğŸ¯ One-Command Addition**: `kin audio add-model` does everything automatically
3. **ğŸ“š Pre-built Templates**: 7 ready-to-use templates for popular models
4. **ğŸ” Auto-Discovery**: System automatically detects Hugging Face repos
5. **ğŸ“¦ Auto-Pull**: Models download automatically when first used
6. **ğŸ› ï¸ Smart Loading**: Server automatically handles different model types
7. **âœ… Validation**: Built-in validation ensures model configs are correct
8. **ğŸš€ Instant Testing**: Run servers immediately after adding models

### ğŸ› ï¸ **Quick Model Addition Methods**

#### **Method 1: Template (Easiest)**
```bash
kin audio add-model --template whisper_stt --name my-whisper
# âœ… Done! Model added, validated, and ready to use
```

#### **Method 2: Direct Repo**
```bash
kin audio add-model --repo openai/whisper-medium --name whisper-med --type stt
# âœ… Done! Custom model added with auto-detection
```

#### **Method 3: Manual JSON (For Advanced Users)**
```json
// Edit models.json directly
{
  "name": "my-custom-model",
  "type": "stt",
  "source": "huggingface",
  "huggingface_repo": "org/model",
  "size_mb": 500
}
```

**Adding a new model to LocalKin Service Audio takes less than 30 seconds!** âš¡ğŸ‰

### ğŸ“Š **Model Addition Statistics**

- **Average Time**: 15-30 seconds per model
- **Success Rate**: 99% for Hugging Face models
- **Auto-Features**: Validation, type detection, size estimation
- **Zero Code Changes**: Most models require no programming
- **Immediate Testing**: Run servers right after adding

---

## ğŸ”§ Advanced Usage

### Batch Processing

```bash
# Process multiple audio files
for file in *.wav; do
  kin audio transcribe "$file" > "${file%.wav}.txt"
done

# Batch TTS generation
echo "First message" > messages.txt
echo "Second message" >> messages.txt
echo "Third message" >> messages.txt

while IFS= read -r message; do
  kin audio tts "$message" --output "tts_$(echo "$message" | head -c 10).wav"
done < messages.txt
```

### Audio Format Support

LocalKin Service Audio supports various audio formats through Whisper:

- **WAV**: Native support
- **MP3**: Requires FFmpeg
- **M4A**: Requires FFmpeg
- **FLAC**: Native support
- **OGG**: Requires FFmpeg

### Quality vs Speed Trade-offs

| Model Size | Quality | Speed | Use Case |
|------------|---------|-------|----------|
| tiny | Basic | 32x | Quick transcription |
| base | Good | 16x | Balanced default |
| small | High | 8x | Quality important |
| medium | Very High | 4x | Professional work |
| large | Excellent | 1x | Best possible quality |

---

## ğŸš€ Performance & Hardware

### Minimum Requirements

- **RAM**: 4GB system RAM
- **Storage**: 500MB free space (for base models)
- **Python**: 3.8+

### Recommended Hardware

#### For Basic Usage (Whisper Tiny/Base)
- **RAM**: 8GB+ system RAM
- **Storage**: 2GB+ free space
- **GPU**: Optional (CPU works fine)

#### For High-Quality Models (Whisper Medium/Large)
- **RAM**: 16GB+ system RAM
- **Storage**: 10GB+ free space
- **GPU**: NVIDIA/AMD GPU with 4GB+ VRAM (recommended)

#### For Ollama Models
- **RAM**: 8GB+ system RAM
- **Storage**: 5GB+ free space per model
- **GPU**: 8GB+ VRAM for larger models

### Platform Support

- âœ… **macOS**: Full support (Intel/Apple Silicon)
- âœ… **Linux**: Full support
- âœ… **Windows**: Full support
- âœ… **CPU-only**: All platforms
- âœ… **GPU acceleration**: CUDA (NVIDIA), MPS (Apple), ROCm (AMD)

---

## ğŸ”§ Troubleshooting

### Common Issues

#### "Model not found" Error
```bash
# Check available models
kin audio models

# For Ollama models, ensure Ollama is running
ollama serve

# Pull the model first
kin audio pull whisper-base
```

#### Kokoro TTS Timeout
```bash
# Kokoro CLI loading may timeout even with cached models
kin audio tts "Hello" --model kokoro-82m
# âŒ "Kokoro CLI loading timed out"

# âœ… Recommended solution: Use API server mode
kin audio run kokoro-82m --port 8003
curl -X POST "http://localhost:8003/synthesize" \
     -H "Content-Type: application/json" \
     -d '{"text": "Hello world", "voice": "af_sarah"}' \
     --output speech.wav
# Returns: Binary WAV file (playable audio!)

# Alternative: Use native TTS immediately
kin audio tts "Hello" --model native
```

#### Audio File Issues
```bash
# Check if file exists and is readable
ls -la audio.wav

# Try converting audio format
ffmpeg -i input.mp3 output.wav
kin audio transcribe output.wav
```

#### Memory Issues
```bash
# Use smaller models
kin audio transcribe audio.wav --model_size tiny

# Close other applications
# Use CPU-only mode if GPU memory is limited
```

#### Permission Issues
```bash
# Ensure proper permissions
chmod +x kin
# or
python -m localkin_service_audio.cli
```

### Debug Mode

```bash
# Enable verbose output
python -c "import logging; logging.basicConfig(level=logging.DEBUG)"
kin audio transcribe audio.wav
```

---

## ğŸŒ REST API Reference

LocalKin Service Audio provides complete REST APIs for both STT and TTS models when running in server mode.

### ğŸš€ Getting Started with API

```bash
# Start an API server
kin audio run whisper-tiny-hf --port 8000

# Interactive API documentation
open http://localhost:8000/docs

# API information
curl http://localhost:8000/
```

### ğŸ“‹ API Endpoints

#### Common Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/` | API server information and available endpoints |
| `GET` | `/health` | Health check and server status |
| `GET` | `/models` | Information about loaded models |
| `GET` | `/docs` | **Interactive API documentation (Swagger UI)** |

#### STT Endpoints (Whisper Models)

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/transcribe` | Upload audio file for transcription |

**Transcription Request:**
```bash
curl -X POST "http://localhost:8000/transcribe" \
     -H "accept: application/json" \
     -H "Content-Type: multipart/form-data" \
     -F "file=@audio.wav" \
     -F "language=en" \
     -F "task=transcribe"
```

**Response:**
```json
{
  "text": "Hello, this is the transcription result.",
  "language": "en",
  "confidence": 0.95
}
```

#### TTS Endpoints (SpeechT5/Bark Models)

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/synthesize` | Generate speech from text (returns WAV audio file) |

**Synthesis Request:**
```bash
curl -X POST "http://localhost:8001/synthesize" \
     -H "Content-Type: application/json" \
     -d '{
       "text": "Hello, this is generated speech!",
       "voice": "af_sarah"
     }' \
     --output speech.wav
```

**Response:** Binary WAV audio file (16-bit PCM, 24kHz for Kokoro, 16kHz for SpeechT5)

**Kokoro Voices:** `af_sarah` (American Female), `am_adam` (American Male), `bf_emma` (British Female), `bm_george` (British Male), `jf_alpha` (Japanese Female), `jm_levi` (Japanese Male), `pf_dora` (Portuguese Female), `pm_alex` (Portuguese Male), `ff_siwis` (French Female)

**âœ… Long Text Support:** Kokoro handles paragraphs and long documents perfectly with consistent quality!

### ğŸ“Š Multiple Servers

Run multiple models on different ports:

```bash
# Terminal 1: STT Server
kin audio run faster-whisper-tiny --port 8000

# Terminal 2: Kokoro TTS Server (High Quality)
kin audio run kokoro-82m --port 8001

# Terminal 3: SpeechT5 TTS Server (Fast)
kin audio run speecht5-tts --port 8002

# Use the APIs
# Transcribe audio
curl -X POST "http://localhost:8000/transcribe" \
     -F "file=@input.wav" > transcription.json

# Generate speech with Kokoro (best quality)
curl -X POST "http://localhost:8001/synthesize" \
     -H "Content-Type: application/json" \
     -d '{"text": "Hello from LocalKin Service Audio!", "voice": "af_sarah"}' \
     --output kokoro_speech.wav

# Long text example (handles paragraphs perfectly)
curl -X POST "http://localhost:8001/synthesize" \
     -H "Content-Type: application/json" \
     -d '{"text": "This is a much longer text to demonstrate the kokoro text-to-speech capabilities. It can handle multiple sentences and paragraphs without any issues. The quality remains excellent throughout longer content.", "voice": "am_adam"}' \
     --output long_speech.wav

# Generate speech with SpeechT5 (faster)
curl -X POST "http://localhost:8002/synthesize" \
     -H "Content-Type: application/json" \
     -d '{"text": "Hello from LocalKin Service Audio!"}' \
     --output speecht5_speech.wav
```

### ğŸ”§ Python API (Future Feature)

```python
from localkin_service_audio.core import LocalKinServiceAudio

# Initialize
audio = LocalKinServiceAudio()

# Speech-to-Text
transcription = audio.transcribe("audio.wav", model="whisper-base")

# Text-to-Speech
audio.synthesize("Hello world", output_file="hello.wav")

# List models
models = audio.list_models()
```

### âš™ï¸ Configuration API

```python
from localkin_service_audio.config import get_models, find_model

# Get all models
models = get_models()

# Find specific model
model = find_model("whisper")

# Get model sizes
sizes = get_model_sizes("whisper")
```

---

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

### Development Setup

```bash
# Clone and setup
git clone https://github.com/LocalKinAI/localkin-service-audio.git
cd localkin-service-audio

# Install uv if not already installed
curl -LsSf https://astral.sh/uv/install.sh | sh

# Create virtual environment and install with uv
uv venv
source .venv/bin/activate  # Linux/macOS
# or on Windows: .venv\Scripts\activate

# Install in development mode with all dependencies
uv pip install -e ".[dev]"

# Run tests
pytest

# Run linting (uv handles the tools automatically)
ruff check localkin_service_audio/
ruff format localkin_service_audio/
```

### Adding New Models

1. **Update `models.json`**: Add model configuration
2. **Implement handler**: Add model-specific logic in appropriate module
3. **Update CLI**: Add command-line options if needed
4. **Add tests**: Create unit tests for the new functionality
5. **Update docs**: Add documentation and examples

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](../LICENSE) file for details.

## ğŸ™ Acknowledgments

- **OpenAI**: For Whisper models and the transformers library
- **SYSTRAN**: For faster-whisper and CTranslate2 optimization
- **Ollama**: For the inspiration and Ollama models
- **Hugging Face**: For the model hub and transformers ecosystem
- **Microsoft**: For SpeechT5 and other TTS models
- **Suno**: For Bark TTS models
- **PyTorch**: For the deep learning framework
- **FastAPI**: For the REST API framework
- **Jinja2**: For web interface templating
- **Bootstrap**: For responsive web UI components
- **uv**: For fast Python package management
- **Community**: For feedback and contributions

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/LocalKinAI/localkin-service-audio/issues)
- **Discussions**: [GitHub Discussions](https://github.com/LocalKinAI/localkin-service-audio/discussions)
- **Web Interface**: Access via `kin web` for interactive usage

---

## ğŸš€ Quick Examples

### Start Your First API Server

```bash
# Install with uv
curl -LsSf https://astral.sh/uv/install.sh | sh
git clone https://github.com/LocalKinAI/localkin-service-audio.git
cd localkin-service-audio
uv venv && source .venv/bin/activate
uv pip install -e .

# Start a Whisper STT server (auto-pulls if needed)
kin audio run whisper-tiny-hf --port 8000

# In another terminal, transcribe audio
curl -X POST "http://localhost:8000/transcribe" \
     -F "file=@your-audio.wav"

# Interactive API docs
open http://localhost:8000/docs
```

### Multiple Models, Multiple Servers

```bash
# Terminal 1: STT Server
kin audio run whisper-base-hf --port 8000

# Terminal 2: TTS Server
kin audio run speecht5-tts --port 8001

# Terminal 3: Use both
# Transcribe audio to text
curl -X POST "http://localhost:8000/transcribe" \
     -F "file=@input.wav" \
     | jq -r '.text' > text.txt

# Generate speech from text
curl -X POST "http://localhost:8001/synthesize" \
     -H "Content-Type: application/json" \
     -d "{\"text\": \"$(cat text.txt)\"}" \
     --output output.wav
```

### Check Status and Cache

```bash
# See all models with status
kin audio models

# Check cache usage
kin audio cache info

# System status
kin audio status
```

---

## ğŸ¯ What's New in v1.0

- âœ… **Faster-Whisper Integration**: Up to 4x faster transcription with CTranslate2 optimization
- âœ… **Dual STT Engines**: OpenAI Whisper + faster-whisper with automatic engine selection
- âœ… **8 Faster-Whisper Models**: Individual model entries for all faster-whisper variants
- âœ… **Enhanced Real-time Listening**: TTS model selection, silence detection, and intelligent caching
- âœ… **REST API Servers**: Run models as complete API services
- âœ… **Modern Web Interface**: Beautiful, responsive web UI for audio processing
- âœ… **Auto-Pull**: Models download automatically when needed
- âœ… **Smart Caching**: Persistent local storage with status tracking
- âœ… **Hugging Face Integration**: Direct integration with HF Hub
- âœ… **Multiple Model Support**: STT + TTS in parallel servers
- âœ… **Interactive Documentation**: Auto-generated API docs
- âœ… **Status Indicators**: Real-time model status with emojis
- âœ… **Process Monitoring**: `kin audio ps` command shows running servers
- âœ… **Model Transparency**: STT/TTS commands show detailed model info and statistics
- âœ… **Easy Model Addition**: `kin audio add-model` command with templates
- âœ… **Model Templates**: 7 pre-built templates for popular models
- âœ… **One-Command Setup**: Add any Hugging Face model in seconds
- âœ… **Enhanced CLI**: Comprehensive model details, file size tracking, progress indicators
- âœ… **Fast Package Management**: uv integration for quick installs
- âœ… **Modular Architecture**: Clean, maintainable codebase with separated concerns
- âœ… **Cross-Platform Compatibility**: Works on macOS, Linux, and Windows

---

## ğŸ¯ Current Capabilities

LocalKin Service Audio v1.0 provides a complete audio AI ecosystem:

### âœ… **Fully Functional Features**
- **ğŸ’» Modern Web Interface**: `kin web` - Beautiful web UI for audio processing
- **ğŸ” Process Monitoring**: `kin audio ps` - See all running servers
- **ğŸ¯ Model Transparency**: STT/TTS commands show detailed model info
- **ğŸš€ REST API Servers**: Run any model as a web service
- **ğŸ“¦ Smart Caching**: Automatic model downloads and storage
- **ğŸ› ï¸ Easy Model Addition**: Add new models in seconds
- **ğŸ“š Model Templates**: 7 pre-built templates for popular models
- **ğŸ”§ Modular Architecture**: Clean separation of core, API, CLI, and UI components

### âœ… **12 Production-Ready Models**
- **4 Hugging Face STT Models**: whisper-tiny, whisper-base, whisper-large-v2, speecht5
- **2 Hugging Face TTS Models**: speecht5-tts, bark-small
- **3 Ollama STT Models**: whisper-large-v3, whisper-base, advanced variants
- **3 Ollama TTS Models**: llama3.2, qwen2.5, mistral conversational models
- **2 Local Models**: OpenAI Whisper (multiple sizes), native TTS

### âœ… **Multiple Usage Patterns**
```bash
# Quick transcription
kin audio transcribe audio.wav

# Web interface (recommended for beginners)
kin web
# Open http://localhost:8080 in your browser

# API server mode
kin audio run whisper-base-hf --port 8000
curl -X POST "http://localhost:8000/transcribe" -F "file=@audio.wav"

# Voice synthesis
kin audio tts "Hello world" --output greeting.wav

# Monitor everything
kin audio ps
kin audio models
kin audio cache info
```

### âœ… **Enterprise-Ready Features**
- **ğŸ”’ Local Processing**: All audio processing happens locally
- **âš¡ High Performance**: GPU acceleration support
- **ğŸ“Š Detailed Logging**: Comprehensive progress and statistics
- **ğŸ›¡ï¸ Error Handling**: Robust error recovery and validation
- **ğŸ“ˆ Scalability**: Multiple servers, multiple models simultaneously

---

## ğŸµ Audio Processing Guide

### Supported Audio Formats

LocalKin Service Audio supports various audio formats through FFmpeg integration:

#### Native Support (No FFmpeg Required)
- **WAV**: Uncompressed PCM audio (recommended)
- **FLAC**: Free Lossless Audio Codec

#### FFmpeg-Required Formats
- **MP3**: MPEG-1 Audio Layer III
- **M4A/AAC**: Advanced Audio Coding
- **OGG**: Ogg Vorbis
- **WMA**: Windows Media Audio

### Audio Quality Guidelines

#### STT (Speech-to-Text) Recommendations
- **Sample Rate**: 16kHz (optimal for Whisper models)
- **Channels**: Mono (stereo files are automatically converted)
- **Bit Depth**: 16-bit PCM
- **Format**: WAV or FLAC

#### TTS (Text-to-Speech) Output
- **Sample Rate**: 22.05kHz (default) or 44.1kHz
- **Channels**: Mono
- **Format**: WAV (uncompressed) or MP3 (compressed)

### Audio Preprocessing

#### Automatic Processing
LocalKin Service Audio automatically handles:
- Sample rate conversion
- Channel mixing (stereo â†’ mono)
- Format conversion
- Normalization

#### Manual Preprocessing (Optional)

```bash
# Convert to optimal format for STT
ffmpeg -i input.mp3 -ac 1 -ar 16000 -c:a pcm_s16le output.wav

# Normalize audio levels
ffmpeg -i input.wav -af "loudnorm" output_normalized.wav
```

### Performance Optimization

#### Hardware Acceleration
- **CPU**: Works on all systems
- **GPU**: NVIDIA CUDA, Apple MPS, AMD ROCm
- **Memory**: 4GB+ RAM recommended

#### Model Selection by Hardware

| Hardware | Recommended Models | Memory Usage |
|----------|-------------------|--------------|
| CPU Only | whisper-tiny, whisper-base | 4GB RAM |
| 4GB GPU | whisper-base, whisper-small | 8GB RAM |
| 8GB+ GPU | whisper-medium, whisper-large | 16GB+ RAM |

### Batch Processing

#### Processing Multiple Files

```bash
# Process all WAV files in directory
for file in *.wav; do
  kin audio transcribe "$file" > "${file%.wav}.txt"
done

# Batch TTS generation
while IFS= read -r line; do
  kin audio tts "$line" --output "tts_$(echo "$line" | head -c 20 | tr ' ' '_').wav"
done < text_lines.txt
```

#### Parallel Processing

```bash
# Run multiple servers simultaneously
kin audio run whisper-tiny-hf --port 8000 &
kin audio run whisper-base-hf --port 8001 &
kin audio run speecht5-tts --port 8002 &

# Use servers in parallel
curl -X POST "http://localhost:8000/transcribe" -F "file=@file1.wav" &
curl -X POST "http://localhost:8001/transcribe" -F "file=@file2.wav" &
```

### Audio Quality Enhancement

#### Noise Reduction
```python
# Using scipy for noise reduction
from scipy.io import wavfile
import noisereduce as nr

rate, data = wavfile.read('noisy_audio.wav')
reduced_noise = nr.reduce_noise(y=data, sr=rate)
wavfile.write('clean_audio.wav', rate, reduced_noise)
```

#### Voice Activity Detection
```python
# Using webrtcvad for VAD
import webrtcvad

vad = webrtcvad.Vad()
vad.set_mode(3)  # Most aggressive filtering

# Process audio frames
# ... VAD implementation ...
```

### Troubleshooting Audio Issues

#### Common Problems

##### "No audio detected" Error
- Check file format and encoding
- Ensure audio file is not corrupted
- Verify sample rate (must be > 8kHz)

##### Poor Transcription Quality
- Use higher quality Whisper models
- Ensure clean audio (minimal background noise)
- Check microphone quality for recordings

##### TTS Audio Issues
- Verify output format compatibility
- Check speaker configuration
- Ensure sufficient disk space

##### Real-time Listening Issues
- **"No microphone found"**: Ensure microphone is connected and permissions granted
- **"Audio device busy"**: Close other audio applications using the microphone
- **TTS model fails**: Command falls back to native TTS automatically
- **TTS model loading timeout**: This is expected with CLI mode. Use API server instead:
  ```bash
  kin audio run kokoro-82m --port 8001  # Start API server
  # Then use API calls for TTS
  ```
- **SpeechT5 SentencePiece error**: Install sentencepiece: `uv add sentencepiece`
- **High CPU usage**: Use faster-whisper models for better performance
- **No response**: Check silence threshold (speak louder or adjust microphone)
- **Variable scoping errors**: Ensure you're using the latest version with TTS cooldown fixes

#### Audio Format Conversion

```bash
# Convert any audio to WAV
ffmpeg -i input.any -acodec pcm_s16le -ar 16000 output.wav

# Convert with specific parameters
ffmpeg -i input.mp3 -ac 1 -ar 22050 -ab 128k output.wav
```

### Advanced Audio Processing

#### Real-time Audio Processing
```python
import pyaudio
import numpy as np

# Real-time audio capture and processing
p = pyaudio.PyAudio()
stream = p.open(format=pyaudio.paInt16,
                channels=1,
                rate=16000,
                input=True,
                frames_per_buffer=1024)

# Process audio chunks
while True:
    data = stream.read(1024)
    # Process audio data...
```

#### Audio Segmentation
```python
# Split long audio files
from pydub import AudioSegment

audio = AudioSegment.from_wav("long_file.wav")
segments = audio[::30000]  # 30 second chunks

for i, segment in enumerate(segments):
    segment.export(f"segment_{i}.wav", format="wav")
```

### Performance Benchmarks

#### STT Performance (Whisper Models)

| Model | Size | Speed | Quality | Memory |
|-------|------|-------|---------|--------|
| tiny | 39MB | 32x | Basic | 1GB |
| base | 74MB | 16x | Good | 1GB |
| small | 244MB | 8x | High | 2GB |
| medium | 769MB | 4x | Very High | 5GB |
| large | 1550MB | 1x | Excellent | 10GB |

#### TTS Performance (Various Models)

| Model | Speed | Quality | Memory |
|-------|-------|---------|--------|
| pyttsx3 | Instant | Good | Minimal |
| SpeechT5 | Medium | High | 250MB |
| Bark | Slow | Very High | 1.7GB |
| XTTS v2 | Medium | Excellent | 1.8GB |

---

## ğŸ› ï¸ Model Management Guide

### Model Architecture

LocalKin Service Audio supports multiple model sources and types:

#### Supported Model Sources
- **Hugging Face Hub**: Direct integration with HF models
- **Ollama Models**: Local Ollama instances
- **Local Models**: Built-in OpenAI Whisper and pyttsx3

#### Model Types
- **STT (Speech-to-Text)**: Whisper variants, Wav2Vec2, HuBERT
- **TTS (Text-to-Speech)**: SpeechT5, Bark, Tacotron2, FastSpeech2

### Model Caching System

#### Cache Location
```
~/.localkin_service_audio/cache/
â”œâ”€â”€ huggingface/     # Hugging Face models
â””â”€â”€ ollama/         # Ollama models
```

#### Cache Management

```bash
# View cache status
kin audio cache info

# Clear specific model
kin audio cache clear whisper-tiny-hf

# Clear all cached models
kin audio cache clear

# Check cache size
du -sh ~/.localkin_service_audio/cache/
```

#### Auto-Pull Behavior

Models are automatically downloaded when first used:

```bash
kin audio run whisper-large-v2-hf --port 8000
# Output:
# ğŸ“¥ Model 'whisper-large-v2-hf' not found in cache. Pulling it first...
# ğŸ“¥ Downloading model from Hugging Face: openai/whisper-large-v2
# âœ… Model downloaded to: /path/to/cache/whisper-large-v2-hf
# âœ… Model pulled successfully!
# ğŸš€ Starting LocalKin Service Audio API server for whisper-large-v2-hf
```

### Adding New Models

#### Method 1: Quick Add (Hugging Face)

```bash
# Add model using template
kin audio add-model --template whisper_stt --name my-whisper

# Add custom Hugging Face model
kin audio add-model --repo openai/whisper-medium --name whisper-med --type stt

# Add with full configuration
kin audio add-model \
  --repo microsoft/speecht5_tts \
  --name speecht5 \
  --type tts \
  --description "Microsoft's advanced neural TTS" \
  --size-mb 1300
```

#### Method 2: Manual JSON Configuration

Edit `localkin_service_audio/core/models.json`:

```json
{
  "name": "custom-whisper-model",
  "type": "stt",
  "description": "Custom Whisper model from Hugging Face",
  "source": "huggingface",
  "huggingface_repo": "organization/model-name",
  "license": "MIT",
  "size_mb": 500,
  "requirements": ["transformers", "torch"],
  "tags": ["custom", "whisper"]
}
```

#### Method 3: Template-Based Addition

Available templates:

```bash
# List all templates
kin audio list-templates

# STT Templates
kin audio add-model --template whisper_stt --name my-whisper
kin audio add-model --template wav2vec2_stt --name my-wav2vec
kin audio add-model --template hubert_stt --name my-hubert

# TTS Templates
kin audio add-model --template speecht5_tts --name my-speecht5
kin audio add-model --template bark_tts --name my-bark
kin audio add-model --template tacotron2_tts --name my-tacotron
```

### Model Configuration

#### Model JSON Schema

```json
{
  "models": [
    {
      "name": "model-name",
      "type": "stt|tts",
      "description": "Human-readable description",
      "source": "huggingface|ollama|openai-whisper|pyttsx3",
      "huggingface_repo": "org/model-name",  // For HF models
      "ollama_model": "model:tag",           // For Ollama models
      "license": "MIT|Apache-2.0|etc",
      "size_mb": 500,
      "requirements": ["package1", "package2"],
      "tags": ["tag1", "tag2"],
      "model_config": {                      // Optional custom config
        "custom_param": "value"
      }
    }
  ]
}
```

#### Validation Rules

- **name**: Required, unique, lowercase with hyphens
- **type**: Required, must be "stt" or "tts"
- **source**: Required, must be valid source type
- **size_mb**: Optional, approximate model size
- **requirements**: Optional, Python packages needed

### Model Server Deployment

#### Single Model Server

```bash
# Start STT server
kin audio run whisper-tiny-hf --port 8000

# Start TTS server
kin audio run speecht5-tts --port 8001

# Check server status
kin audio ps
```

#### Multiple Model Servers

```bash
# Start multiple servers
kin audio run whisper-tiny-hf --port 8000 &
kin audio run whisper-base-hf --port 8001 &
kin audio run speecht5-tts --port 8002 &

# Monitor all servers
kin audio ps
```

#### Production Deployment

```bash
# Use production settings
export LOCALKIN_AUDIO_ENV=production

# Start with custom host
kin audio run whisper-base-hf --host 0.0.0.0 --port 8000

# Use with reverse proxy (nginx)
# Configure nginx to proxy to localhost:8000
```

### Model Performance Tuning

#### Memory Optimization

```python
# Enable memory optimizations
import torch
torch.cuda.empty_cache()  # Clear GPU memory

# Use gradient checkpointing for large models
# (automatically enabled for models > 1GB)
```

#### GPU Acceleration

```bash
# Force CPU usage
export CUDA_VISIBLE_DEVICES=""

# Use specific GPU
export CUDA_VISIBLE_DEVICES="0"

# Check GPU memory usage
nvidia-smi
```

#### Batch Processing Optimization

```python
# Process multiple audio files efficiently
batch_size = 4  # Adjust based on available memory
# Implementation handles batching automatically
```

### Monitoring and Maintenance

#### Server Monitoring

```bash
# Check running processes
kin audio ps

# View detailed server info
curl http://localhost:8000/

# Health check
curl http://localhost:8000/health
```

#### Log Management

```bash
# Enable verbose logging
kin audio run model-name --verbose

# View logs
tail -f ~/.localkin_service_audio/logs/server.log
```

#### Performance Metrics

```python
# Get model performance stats
response = requests.get("http://localhost:8000/metrics")
print(response.json())
# Output: {"requests_per_second": 10.5, "avg_response_time": 0.8}
```

### Troubleshooting Model Issues

#### Common Problems

##### Model Download Failures
```bash
# Check internet connection
ping huggingface.co

# Clear cache and retry
kin audio cache clear model-name
kin audio run model-name --port 8000

# Manual download
git lfs clone https://huggingface.co/org/model-name
```

##### Out of Memory Errors
```bash
# Use smaller model
kin audio run whisper-tiny-hf --port 8000

# Increase system memory
# Or use CPU-only mode
export CUDA_VISIBLE_DEVICES=""
```

##### Model Loading Errors
```bash
# Check model configuration
kin audio models

# Validate model file
python -c "from transformers import pipeline; pipeline('automatic-speech-recognition', model='org/model-name')"

# Reinstall dependencies
pip install --upgrade transformers torch
```

#### Model Validation

```bash
# Test model functionality
curl -X POST "http://localhost:8000/transcribe" \
  -F "file=@test.wav"

# Expected response:
# {"text": "Hello, this is a test", "language": "en", "confidence": 0.95}
```

### Advanced Configuration

#### Custom Model Loading

```python
# For models needing special handling
from localkin_service_audio.core.model_loader import CustomModelLoader

class MyModelLoader(CustomModelLoader):
    def load_model(self, model_config):
        # Custom loading logic
        pass

    def preprocess_audio(self, audio_data):
        # Custom preprocessing
        pass
```

#### Model Registry

```python
# Access model registry
from localkin_service_audio.config import get_models, find_model

models = get_models()
whisper_model = find_model("whisper")
```

### Best Practices

1. **Cache frequently used models** to reduce startup time
2. **Use appropriate model sizes** for your hardware constraints
3. **Monitor memory usage** when deploying multiple models
4. **Implement proper error handling** for production deployments
5. **Regularly update models** to benefit from improvements
6. **Use model validation** before deploying to production
7. **Implement logging and monitoring** for production systems

---

## âš¡ uv Setup Guide

### Performance Benefits
- **10-100x faster** than pip for dependency resolution
- **Parallel downloads** for faster installations
- **Smart caching** - dependencies only downloaded once

### Reliability
- **Reproducible builds** with lock files
- **Better dependency resolution** avoids conflicts
- **Atomic operations** - no partial installs

### Modern Python Packaging
- **pyproject.toml** as the single source of truth
- **Built-in tools** for linting, formatting, testing
- **Virtual environment management** built-in

### Available Commands

```bash
# Package management
uv pip install -e .                    # Install in development mode
uv pip install -e ".[dev]"            # Install with dev dependencies
uv pip install -e ".[web]"            # Install with web UI support
uv pip install -e ".[tts]"            # Install with advanced TTS support
uv pip install -e ".[gpu]"            # Install with GPU support
uv venv                               # Create virtual environment
uv sync                               # Sync dependencies

# Development tools
uv run pytest                         # Run tests
uv run ruff check localkin_service_audio/        # Lint code
uv run ruff format localkin_service_audio/       # Format code
uv run mypy localkin_service_audio/              # Type check
```

### Why uv?

#### Before (pip):
```bash
pip install -r requirements.txt  # Slow, potential conflicts
pip install -e .                # Manual dependency management
```

#### After (uv):
```bash
uv pip install -e .             # Fast, reliable, conflict-free
uv sync                        # Perfect dependency resolution
```

#### Performance Comparison:
- **Installation**: 10-100x faster
- **Dependency resolution**: Much more reliable
- **Caching**: Only download once, reuse everywhere

---

## ğŸ“‹ Changelog

### [1.0.0] - 2024-12-XX

#### Added
- ğŸš€ **REST API Servers**: Run models as complete API services with auto-generated documentation
- ğŸ“¦ **Smart Caching**: Automatic model downloads and persistent local storage
- ğŸ” **Process Monitoring**: `kin audio ps` command to monitor running servers
- ğŸ¯ **Model Transparency**: Detailed model information and statistics in STT/TTS commands
- ğŸ› ï¸ **Easy Model Addition**: `kin audio add-model` command with 7 pre-built templates
- ğŸ¨ **Multiple Model Support**: Run STT and TTS models simultaneously on different ports
- ğŸ“Š **Status Indicators**: Real-time model status with emoji indicators
- âš¡ **Fast Package Management**: uv integration for quick installs
- ğŸŒ **Hugging Face Integration**: Direct integration with HF Hub for model management
- ğŸµ **XTTS v2 Support**: High-quality multilingual voice cloning TTS
- ğŸŒ **Modern Web Interface**: Beautiful, responsive web UI for easy audio processing

#### Supported Models
- **STT Models**: OpenAI Whisper (multiple sizes), faster-whisper, Hugging Face transformers
- **TTS Models**: Microsoft SpeechT5, Suno Bark, HexGrad Kokoro, Coqui XTTS v2, native OS TTS
- **API Server Support**: All Hugging Face models can be run as REST APIs

#### Features
- **Auto-Pull**: Models download automatically when needed
- **Interactive Documentation**: Auto-generated API docs at `/docs`
- **Batch Processing**: Process multiple audio files
- **Audio Format Support**: WAV, MP3, M4A, FLAC, OGG
- **GPU Acceleration**: CUDA, MPS, and CPU support
- **Cross-Platform**: macOS, Linux, Windows
- **Voice Cloning**: XTTS v2 with multilingual support

### [0.1.0] - 2024-01-XX

#### Added
- Initial release with basic STT and TTS functionality
- Local Whisper model support
- Native OS TTS integration
- Basic CLI interface

---

**ğŸ‰ Ready to get started with local audio AI?** Install LocalKin Service Audio and choose your preferred interface! ğŸµâœ¨

```bash
# Option 1: Web Interface (Recommended for beginners)
kin web
# ğŸŒ Open http://localhost:8080 in your browser

# Option 2: API Server
kin audio run whisper-tiny-hf --port 8000
# ğŸš€ API server ready at http://localhost:8000

# Option 3: CLI Commands
kin audio transcribe audio.wav
# ğŸ“ Direct transcription to terminal
```
